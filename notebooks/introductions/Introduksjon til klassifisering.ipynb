{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduksjon til klassifisering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hva er klassifisering?\n",
    "Klassifisering er en form for maskinlæringsproblem der man ønsker å trene maskinen til å finne hvilken kategori (*klasse*) noe hører til. Tenk for eksempel at du er en  maskin og jeg skal trene deg til å plukke ut bøker som jeg liker. Det første vi må gjøre er å representere bøkene med tall. For eksempel kan jeg ha hvor skummel en bok er på x aksen og hvor morsom den er på y aksen. Da kan vi plotte det opp i et scatterplot slik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importere read_csv fra pandas pakken\n",
    "from pandas import *\n",
    "# importere pakker for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# bruke read_csv til å lese inn tabellen\n",
    "data = read_csv('../../datasets/small_examples/books.csv') # Lese inn data\n",
    "\n",
    "# lagre hver av klassene i egen variabel.\n",
    "# dette er egentlig ikke nødvendig, men det gjør det lettere å tegne \n",
    "# merkelapper for hver klasse\n",
    "data_klasse1 = data.loc[data['Klasse'] == 0] \n",
    "data_klasse2 = data.loc[data['Klasse'] == 1]\n",
    "\n",
    "# plotte bøkene jeg ikke liker som rød sirkler med merkelappen \"Liker ikke\"\n",
    "plt.scatter(data_klasse1['Skummel'], data_klasse1['Morsom'], c='r', label='Liker ikke')\n",
    "\n",
    "# plotte bøkene jeg liker som blå sirkler med merkelappen \"Liker\"\n",
    "plt.scatter(data_klasse2['Skummel'], data_klasse2['Morsom'], c='b', label='Liker')\n",
    "\n",
    "# lable aksene\n",
    "plt.xlabel('Skummel')\n",
    "plt.ylabel('Morsom')\n",
    "\n",
    "# tegne opp merkelapper for klassene\n",
    "plt.legend()\n",
    "\n",
    "# vise frem figuren\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De blå punktene er bøker som jeg liker og de røde punktene er bøker jeg ikke liker. Du ser kanskje at det er et mønster her? La oss si at jeg kommer med en ny, ukjent bok som er middels skummel og veldig lite morsom. (Representert i plottet under som et spørsmåltegn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lage en ny figure\n",
    "plt.figure()\n",
    "\n",
    "# plotte bøkene jeg ikke liker som rød sirkler med merkelappen \"Liker ikke\"\n",
    "plt.scatter(data_klasse1['Skummel'], data_klasse1['Morsom'], c='r', label='Liker ikke')\n",
    "\n",
    "# plotte bøkene jeg liker som blå sirkler med merkelappen \"Liker\"\n",
    "plt.scatter(data_klasse2['Skummel'], data_klasse2['Morsom'], c='b', label='Liker')\n",
    "\n",
    "# plotte en ny bok representert av et spørsmåltegn\n",
    "plt.scatter(2.5,2, c='k',s=300,marker='$?$')\n",
    "\n",
    "# lable aksene\n",
    "plt.xlabel('Skummel')\n",
    "plt.ylabel('Morsom')\n",
    "\n",
    "# tegne opp merkelapper for klassene\n",
    "plt.legend()\n",
    "\n",
    "# vise frem figuren\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Oppgave 1:** Tror du at jeg kommer til å like eller ikke like boken? Hvorfor? Fyll inn svar i cellen under"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Ditt svar her:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 2:** Hva om jeg har en bok representert av punktet (2.0,3.6)? Kommer jeg til å like eller ikke like boken?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Ditt svar her:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men å se på plottet og avgjøre manuelt hvilken klasse et eksempel tilhører, er ikke spesielt presist. Det vi ønsker er å finne en måte for maskinen å lære å bestemme klassen til et eksempel basert på egenskaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nærmeste Nabo\n",
    "\n",
    "En enkel måte å finne hvilken klasse et eksempel tilhører, er å se på *det nærmeste kjente eksempelet*. Det er rimelig å anta at ting som er like har samme klasse. Hvis jeg for eksempel finner en ny bok, og den er ganske lik som en annen bok jeg liker, er det sannsynlig at jeg vil like den nye boka også. \n",
    "Så en mulig taktikk for å klassifisere et nytt punkt, `x_ny`, er å først finne det nærmeste andre punktet, `x_nabo`. Så kan vi sjekke hvilken klasse det har, og så si at klassen til `x_ny` er den samme som klassen til `x_nabo`. Med andre ord: Hvis den nye boka ligner mest på en bok jeg vet jeg liker, så gjetter vi at jeg liker den nye boka også! Denne algoritmen kaller vi *Nærmeste Nabo*\n",
    "\n",
    "**Oppgave 3:** Ta en titt på figuren over igjen. Hvilket punkt er den nærmeste naboen til det ukjente punktet? Hvilken klasse får det ukjente punktet da?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Ditt svar her:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nærmeste Naboer\n",
    "Hva om det plutselig er en middels skummel og lite morsom bok som jeg plutselig liker, selv om jeg vanligvis ikke gjør det? Da kan plottet vårt for eksempel se slikt ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lage en ny figure\n",
    "plt.figure()\n",
    "\n",
    "# Endre et klassen til et datapunkt\n",
    "data.loc[8]['Klasse'] = 1\n",
    "\n",
    "# hente ut hver klasse i egen variabel for plotting\n",
    "data_klasse1 = data.loc[data['Klasse'] == 0] \n",
    "data_klasse2 = data.loc[data['Klasse'] == 1]\n",
    "\n",
    "# plotte bøkene jeg ikke liker som rød sirkler med merkelappen \"Liker ikke\"\n",
    "plt.scatter(data_klasse1['Skummel'], data_klasse1['Morsom'], c='r', label='Liker ikke')\n",
    "\n",
    "# plotte bøkene jeg liker som blå sirkler med merkelappen \"Liker\"\n",
    "plt.scatter(data_klasse2['Skummel'], data_klasse2['Morsom'], c='b', label='Liker')\n",
    "\n",
    "# plotte en ny bok representert av et spørsmåltegn\n",
    "plt.scatter(2.5,2,c='k',s=300,marker='$?$')\n",
    "\n",
    "# lable aksene\n",
    "plt.xlabel('Skummel')\n",
    "plt.ylabel('Morsom')\n",
    "\n",
    "# tegne opp merkelapper for klassene\n",
    "plt.legend()\n",
    "\n",
    "# vise frem figuren\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Oppgave 4: ** Hvilken klasse vil spørsmåltegnet få nå? Virker det rimelig?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Ditt svar her*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi ser er Nærmeste Nabo algoritmen følsom for \"støy\" i dataene. En bok som er litt annerledes vil påvirke anbefalingen av fremtidige bøker. En måte å komme rundt dette på, er å **spørre mer enn en nabo.** Hvis vi for eksempel spør de tre nærmeste naboene, vil vi i dette tilfellet få (\"liker\", \"liker ikke\", \"liker ikke\"). Det er et flertall av \"liker ikke\", så vi setter `x_ny` til å også få klassen \"liker ikke\". Dette kalles *K nærmeste nabo algoritmen* hvor k er antallet nærmeste naboer vi spør. Det er gjerne sånn at dersom vi spør flere naboer, så blir vi sikrere, men hvis vi spør for mange kan det også gå galt. \n",
    "\n",
    "I det mest ekstreme tilfellet vil vi spørre alle punktene og da vil vi alltid ende opp med klassen som flest punkter tilhører. I vårt eksempel vil alle bøker få klassen \"liker\" siden det er flest punkter i den klassen. \n",
    "Det å finne den riktige balansen mellom høy og lav k er et eksempel på en av de viktigste utfordringene i maskinlæring, nemlig valg av parametere. Vi skal snakke mer om det siden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN i kode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis du har kjennskap til programmering fra før har du kanskje begynt å tenke hvordan KNN kan skrives i kode. Jeg anbefaler alle som har lyst til å prøve å implementere algoritmen selv til å gjøre det. Det er veldig lærerikt og alt man trenger å gjøre er å søke gjennom alle punkter, finne de k nærmeste punktene og telle opp hvilken klasse som har flertall blant de. \n",
    "\n",
    "Men i dette kurset skal vi bruke scikit learn sin ferdige klasse for K Nærmeste Naboer. Den brukes veldig likt som vi gjorde for lineær regresjon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # importere klassifikator \n",
    "clf = KNeighborsClassifier(n_neighbors=3) # Lage KNN klassifikator med k=3\n",
    "clf.fit(data[['Skummel','Morsom']],data['Klasse']) # Trene klassifikatoren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi har trent klassifikatoren med `clf.fit(x,y)` kan vi klassifisere nye eksempler med `clf.predict(x_ny)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[2.5,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som forventet ble det ukjente punktet klassifisert som `0` (\"ikke liker\") av KNN med `k=3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overtilpassing\n",
    "\n",
    "Akkurat som for regresjon, kan *overtilpassing* være et problem. Spesielt hvis eksemplene overlapper litt. Sett at vi har følgende data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag ny figur\n",
    "plt.figure()\n",
    "\n",
    "# les inn eksempeldata\n",
    "example_data = read_csv('../../datasets/small_examples/example1.csv')\n",
    "\n",
    "# hvis eksempeldata i et scatterplott\n",
    "plt.scatter(example_data['0'],example_data['1'],c=example_data['2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvilken av de to skillene under tror du passer best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi må importere litt for å kunne bruke hjelpefunksjoner\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.split(os.path.abspath(os.getcwd()))[0])\n",
    "from useful_tools import plot_boundary\n",
    "\n",
    "# opprette en ny figur som er 12 i bredden og 4 i høyden\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Opprette en KNN klassifikator med k=1\n",
    "clf1 = KNeighborsClassifier(n_neighbors = 1)\n",
    "# trene klassifikatoren\n",
    "clf1.fit(example_data[['0','1']],example_data['2'])\n",
    "# opprette et subplot for å ha to plott ved siden av hverandre\n",
    "plt.subplot(1,2,1)\n",
    "# plot_boundary er en hjelpefunksjon som plotter beslutnigsgrensa for en modell\n",
    "plot_boundary(example_data[['0','1']],clf1,padding=0.1,plot_step=0.005)\n",
    "# tegn opp datapunktene\n",
    "plt.scatter(example_data['0'],example_data['1'],c=example_data['2'])\n",
    "plt.title('Beslutningsgrense 1')\n",
    "\n",
    "# Opprette en KNN klassifikator med k=10\n",
    "clf2 = KNeighborsClassifier(n_neighbors = 10)\n",
    "# trene klassifikatoren\n",
    "clf2.fit(example_data[['0','1']],example_data['2'])\n",
    "# opprette et subplot for å ha to plott ved siden av hverandre\n",
    "plt.subplot(1,2,2)\n",
    "# plot_boundary er en hjelpefunksjon som plotter beslutnigsgrensa for en modell\n",
    "plot_boundary(example_data[['0','1']],clf2,padding=0.1,plot_step=0.005)\n",
    "# tegn opp datapunktene\n",
    "plt.scatter(example_data['0'],example_data['1'],c=example_data['2'])\n",
    "plt.title('Beslutningsgrense 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beslutningsgrense 1 passer perfekt til dataen, men den er veldig detaljert og kronglete. Den virker ikke som den *generaliserer bra til ukjent data*. Beslutningsgrense 2 passer litt dårligere men den er mer generell. \n",
    "\n",
    "Så for å måle hvor bra en klassifiseringsmodell er, er det svært viktig at vi deler inn i trening og testdata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 5:** Bruk alt du har lært om KNN til å klassifisere et nytt datasett\n",
    "\n",
    "Begynn med å lese inn data fra `'../../datasets/small_examples/example2.csv'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitt inn i trening og testdata med `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### din kode her: ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag en klassifikator med `KNeighborsClassifier` og tren den på **trenigsdata** med `.fit(x,y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### din kode her: ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag en figur som viser treningsdata som sirkler og testdata som kryss. Vis også frem beslutningsgrensa til klassifikatoren din i samme plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### din kode her: ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruk `.predict` til å finne klassene til testdata. Print ut trenings og testnøyaktighet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "###### din kode her: ######\n",
    "\n",
    "###### din kode slutt: ######\n",
    "\n",
    "print(\"treningsnøyaktighet:\", np.mean(clf.predict(train_data)==train_target))\n",
    "print(\"testnøyaktighet:\", np.mean(clf.predict(test_data)==test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 6:** Prøv å kjøre cellene på nytt med andre verdier av k **(hint: `n_neighbors`)**. Prøv for eksempel k = 1, 10, 30 og 67. Hvilken verdi gir størst testnøyaktighet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skriv ditt svar her:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 7:** Hvilken effekt har det på beslutningsgrensa? Hvilken sammenheng har det med test nøyaktigheten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skriv ditt svar her:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistisk regresjon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nærmeste nabo er bare en av mange algoritmer for å klassifisere data basert på egenskaper.\n",
    "En annen måte å gjøre klassifisering på er å bruke regresjon litt sånn som vi har sett på tidligere i kurset. La oss se på et 1 dimensjonalt eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi lager litt eksempel data for plotting\n",
    "x = np.array([0.75,0.1,0.3,0.4,0.6,1.1,1.4,1.7,1.9])\n",
    "y = np.array([0,  0,  0,  0,  0,1,  1,1,  1])\n",
    "\n",
    "# henter ut hver klasse i egne variable for plotting\n",
    "x0 = x[y==0]\n",
    "y0 = y[y==0]\n",
    "\n",
    "x1 = x[y==1]\n",
    "y1 = y[y==1]\n",
    "\n",
    "# plotte data\n",
    "plt.figure()\n",
    "plt.plot(x0,y0,'ro')\n",
    "plt.plot(x1,y1,'bo')\n",
    "plt.axis([0,2,-0.1,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her er alle eksemplene med klasse \"RØD\" satt 0 på y aksen og eksemplene med klasse \"BLÅ\" er satt til 1. Vi ønsker å finne hvor skillet mellom klassene går. Den enkleste løsningen vil være linær regresjon. Alt vi trenger å gjøre er å tilpasse en linje til punktene som før. Så kan vi *terskle* linja. Det betyr at vi kan si at der linja er over en hvis grense. f.eks 0.5 er det klasse \"BLÅ\" og ellers er det klasse \"RØD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importere LinearRegression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# lage og trene modellen\n",
    "reg = LinearRegression()\n",
    "reg.fit(x[:,np.newaxis],y[:,np.newaxis])\n",
    "\n",
    "# lage en linje for plotting\n",
    "t = np.linspace(0,2,1001)\n",
    "l = reg.predict(t[:,np.newaxis])\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.axis([0,2,-0.1,1.1]) # justere aksene\n",
    "plt.plot(t,l>=0.5,'m-')\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet lin. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis man nå vil finne klassen til et nytt punkt, f.eks. 1.2, sjekker man først hvilken verdi man får for den grønne linja. I vårt tilfelle er det ca. 0.7. Så sjekker man om det er over terskelen på 0.5. Hvilket det er. Da bestemmer man at klassen er \"BLÅ\". \n",
    "\n",
    "Eventuelt kan man se direkte på den lilla linja som representerer resultatet av tersklingen. Matematisk blir dette det samme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et problem er at linja passer ganske dårlig til dataene og vil bli veldig forskjellig allerede hvis vi legger til et lite punkt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.75,0.1,0.3,0.4,0.6,1.1,1.4,1.7,1.9,2.1])\n",
    "y = np.array([0,  0,  0,  0,  0,1,  1,1,  1,1])\n",
    "\n",
    "x0 = x[y==0]\n",
    "y0 = y[y==0]\n",
    "\n",
    "x1 = x[y==1]\n",
    "y1 = y[y==1]\n",
    "\n",
    "# lage og trene modellen\n",
    "reg = LinearRegression()\n",
    "reg.fit(x[:,np.newaxis],y[:,np.newaxis])\n",
    "\n",
    "# lage en linje for plotting\n",
    "t = np.linspace(0,2.2,1001)\n",
    "l = reg.predict(t[:,np.newaxis])\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.axis([0,2.2,-0.1,1.1]) # justere aksene\n",
    "plt.plot(t,l>=0.5,'m-')\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet lin. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi vil helst ikke ha en modell som er så lett påvirket av små endringer i dataene. Heldigvis er det lett å fikse dette problemet. Vi bytter ut den lineære funksjonen med en *logistisk funksjon*. \n",
    "Denne terskler vi også på 0.5. En logistisk funksjon passer mye bedre med klassifikasjonsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "reg2 = LogisticRegression(C=1e5)\n",
    "reg2.fit(x[:,np.newaxis],y)\n",
    "\n",
    "# Logistisk funksjon er litt utenfor dette kursets tema, så vi har laget en ferdig \n",
    "# matematisk funksjon for det. Du trenger ikke å forstå detaljene her\n",
    "def logistic_model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "loss = logistic_model(t * reg2.coef_ + reg2.intercept_).ravel()\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.plot(t,loss>=0.5,'c-') # plotte terskel\n",
    "plt.plot(t, loss, color='orange', linewidth=3) # plotte logistisk funksjon\n",
    "\n",
    "plt.axis([0,2.2,-0.1,1.1]) # justere aksene\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet log. reg.','log. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistisk regresjon for flere dimensjoner\n",
    "Hittil har vi kun sett på logistisk regresjon klassifisering for en 1-dimensjonal egenskap. Men i praksis har vi gjerne flere egenskaper. Prinsippet er det samme, men i stedet for å tilpasse en linje tilpasser man en høyere dimensjons grense, for eksempel et plan. Under er en figur som viser en \n",
    "logistisk regresjons-grense i to dimensjoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skru på interaktive plot\n",
    "% matplotlib notebook \n",
    "from mpl_toolkits.mplot3d import Axes3D # importere verktøy for 3d plot\n",
    "\n",
    "# lage og trene klassifikator\n",
    "clf_log = LogisticRegression(C=1e5)\n",
    "clf_log.fit(example_data[['0','1']],example_data['2'])\n",
    "\n",
    "# lage ny figur og tegne 3d scatterplot\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(example_data['0'][example_data['2']==0], example_data['1'][example_data['2']==0],\n",
    "           example_data['2'][example_data['2']==0],facecolors='r')\n",
    "ax.scatter(example_data['0'][example_data['2']==1], example_data['1'][example_data['2']==1],\n",
    "           example_data['2'][example_data['2']==1],facecolors='b')\n",
    "\n",
    "# regne ut beslutningsplanet\n",
    "x = y = np.arange(-4, 4, 0.5)\n",
    "X, Y  = np.meshgrid(x, y)\n",
    "def model2d(c,b,x,y):\n",
    "    return logistic_model(c[0]*x + c[1]*y + b)\n",
    "Z = model2d(clf_log.coef_[0],clf_log.intercept_,X,Y)\n",
    "\n",
    "# plotte beslutningsplanet som en wireframe\n",
    "ax.plot_wireframe(X, Y, Z,color ='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når du terskler beslutningplanet blir det som å kutte med en kniv. Det kuttet du får da, blir beslutningsgrensa for 2 dimensjoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skru av interaktive plot\n",
    "% matplotlib inline\n",
    "\n",
    "plt.figure() # ny figur\n",
    "plot_boundary(example_data[['0','1']],clf_log,padding=0.1,plot_step=0.005) # tegne grense\n",
    "plt.scatter(example_data[['0']], example_data[['1']], marker='o', c=example_data['2']) # tegne scatterplot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden logistisk regresjon alltid terskler, hvilket fungerer som et slags \"kutt\", så vil beslutningsgrensa **alltid bli en rett strek**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistisk regresjon i kode\n",
    "\n",
    "Scikit learn har en ferdig pakke for logistisk regresjon også. Den heter `LogisticRegression` og importeres fra `sklearn.linear_model`. Du kan lage en logistisk regresjon klassifikator med `LogisticRegression()` og trene den som før med `.fit(x,y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 8** Tren en logistisk regresjon kode på irisdatasettet. \n",
    "\n",
    "*Irisdatasettet er et datasett med egenskaper for klassifisering av irisblomster. Datasettet ble først introdusert av en biolog og matematiker ved navn Ronald Fisher i 1936 og har siden den gang blitt brukt gjentatte ganger for å teste og visualisere maskinlæringsteknikker. Hvis du vil vite mer om irisdatasettet kan du lese mer <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">her</a>. Nå er det din tur til å gå i generasjoner med maskinlæringsekseperter sine fotspor og prøve å klassifisere irisblomstene ut fra lengde og bredde på kronbladene *\n",
    "\n",
    "**Hint:**\n",
    "Første steg er å lese inn datasettet fra `'..\\..\\datasets\\iris-species\\Iris.csv'`og hente ut kollonene med egenskapene `SepalWidthCm` og `PetalLengthCm`. Du må også hente ut kollonen `'Species'` som inneholder klassen. Så må du splitte opp i trening og testdata. Deretter er koden ganske lik som det vi gjorde for KNN, men husk å importere og bruke LogisticRegression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### din kode her: ######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Oppgave 9** Kjør koden fra oppgave 8 på nytt, men test ut andre par med egenskaper. Hvilke skiller best ut iristypene?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
