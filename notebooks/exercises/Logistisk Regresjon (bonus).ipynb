{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistisk regresjon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidligere har vi sett på klassifisering med *K Nærmeste Naboer*. Men KNN er bare en av mange algoritmer for å klassifisere data basert på tallegenskaper.\n",
    "En annen måte å gjøre klassifisering på er å bruke regresjon litt sånn som vi så på på starten av kurset.  La oss først se på et 1 dimensjonalt eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi lager litt eksempel data for plotting\n",
    "x = np.array([0.75,0.1,0.3,0.4,0.6,1.1,1.4,1.7,1.9])\n",
    "y = np.array([0,  0,  0,  0,  0,1,  1,1,  1])\n",
    "\n",
    "# henter ut hver klasse i egne variable for plotting\n",
    "x0 = x[y==0]\n",
    "y0 = y[y==0]\n",
    "\n",
    "x1 = x[y==1]\n",
    "y1 = y[y==1]\n",
    "\n",
    "# plotte data\n",
    "plt.figure()\n",
    "plt.plot(x0,y0,'ro')\n",
    "plt.plot(x1,y1,'bo')\n",
    "plt.axis([0,2,-0.1,1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her er alle eksemplene med klasse \"RØD\" satt 0 på y aksen og eksemplene med klasse \"BLÅ\" er satt til 1. Vi ønsker å finne hvor skillet mellom klassene går. Den enkleste løsningen vil være linær regresjon. Alt vi trenger å gjøre er å tilpasse en linje til punktene som før. Så kan vi *terskle* linja. Det betyr at vi kan si at der linja er over en hvis grense. f.eks 0.5 er det klasse \"BLÅ\" og ellers er det klasse \"RØD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importere LinearRegression\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# lage og trene modellen\n",
    "reg = LinearRegression()\n",
    "reg.fit(x[:,np.newaxis],y[:,np.newaxis])\n",
    "\n",
    "# lage en linje for plotting\n",
    "t = np.linspace(0,2,1001)\n",
    "l = reg.predict(t[:,np.newaxis])\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.axis([0,2,-0.1,1.1]) # justere aksene\n",
    "plt.plot(t,l>=0.5,'m-')\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet lin. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis man nå vil finne klassen til et nytt punkt, f.eks. 1.2, sjekker man først hvilken verdi man får for den grønne linja. I vårt tilfelle er det ca. 0.7. Så sjekker man om det er over terskelen på 0.5. Hvilket det er. Da bestemmer man at klassen er \"BLÅ\". \n",
    "\n",
    "Eventuelt kan man se direkte på den lilla linja som representerer resultatet av tersklingen. Matematisk blir dette det samme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et problem er at linja passer ganske dårlig til dataene og vil bli veldig forskjellig allerede hvis vi legger til et lite punkt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.75,0.1,0.3,0.4,0.6,1.1,1.4,1.7,1.9,2.1])\n",
    "y = np.array([0,  0,  0,  0,  0,1,  1,1,  1,1])\n",
    "\n",
    "x0 = x[y==0]\n",
    "y0 = y[y==0]\n",
    "\n",
    "x1 = x[y==1]\n",
    "y1 = y[y==1]\n",
    "\n",
    "# lage og trene modellen\n",
    "reg = LinearRegression()\n",
    "reg.fit(x[:,np.newaxis],y[:,np.newaxis])\n",
    "\n",
    "# lage en linje for plotting\n",
    "t = np.linspace(0,2.2,1001)\n",
    "l = reg.predict(t[:,np.newaxis])\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.axis([0,2.2,-0.1,1.1]) # justere aksene\n",
    "plt.plot(t,l>=0.5,'m-')\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet lin. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.75,0.1,0.3,0.4,0.6,1.1,1.4,1.7,1.9,2.1])\n",
    "y = np.array([0,  0,  0,  0,  0,1,  1,1,  1,1])\n",
    "\n",
    "x0 = x[y==0]\n",
    "y0 = y[y==0]\n",
    "\n",
    "x1 = x[y==1]\n",
    "y1 = y[y==1]\n",
    "\n",
    "# lage og trene modellen\n",
    "reg = LinearRegression()\n",
    "reg.fit(x[:,np.newaxis],y[:,np.newaxis])\n",
    "\n",
    "# lage en linje for plotting\n",
    "t = np.linspace(0,2.2,1001)\n",
    "l = reg.predict(t[:,np.newaxis])\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.axis([0,2.2,-0.1,1.1]) # justere aksene\n",
    "plt.plot(t,l>=0.5,'m-')\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet lin. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "reg2 = LogisticRegression(C=1e5)\n",
    "reg2.fit(x[:,np.newaxis],y)\n",
    "\n",
    "# Logistisk funksjon er litt utenfor dette kursets tema, så vi har laget en ferdig \n",
    "# matematisk funksjon for det. Du trenger ikke å forstå detaljene her\n",
    "def logistic_model(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "loss = logistic_model(t * reg2.coef_ + reg2.intercept_).ravel()\n",
    "\n",
    "plt.figure() # opprette ny figur\n",
    "plt.plot(t,l,'g-') # tegne modellen som linje\n",
    "plt.plot(x0,y0,'ro') # tegne den røde klassen\n",
    "plt.plot(x1,y1,'bo') # tegne den blå klassen \n",
    "plt.plot(t,loss>=0.5,'c-') # plotte terskel\n",
    "plt.plot(t, loss, color='orange', linewidth=3) # plotte logistisk funksjon\n",
    "\n",
    "plt.axis([0,2.2,-0.1,1.1]) # justere aksene\n",
    "plt.legend(['lin. reg.','roed','blaa','tersklet log. reg.','log. reg.'],loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistisk regresjon for flere dimensjoner\n",
    "Hittil har vi kun sett på logistisk regresjon klassifisering for en 1-dimensjonal egenskap. Men i praksis har vi gjerne flere egenskaper. Prinsippet er det samme, men i stedet for å tilpasse en linje tilpasser man en høyere dimensjons grense, for eksempel et plan. Under er en figur som viser en \n",
    "logistisk regresjons-grense i to dimensjoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# les inn eksempeldata\n",
    "example_data = pd.read_csv('../../datasets/small_examples/example1.csv')\n",
    "\n",
    "# skru på interaktive plot\n",
    "% matplotlib notebook \n",
    "from mpl_toolkits.mplot3d import Axes3D # importere verktøy for 3d plot\n",
    "\n",
    "# lage og trene klassifikator\n",
    "clf_log = LogisticRegression(C=1e5)\n",
    "clf_log.fit(example_data[['0','1']],example_data['2'])\n",
    "\n",
    "# lage ny figur og tegne 3d scatterplot\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(example_data['0'][example_data['2']==0], example_data['1'][example_data['2']==0],\n",
    "           example_data['2'][example_data['2']==0],facecolors='r')\n",
    "ax.scatter(example_data['0'][example_data['2']==1], example_data['1'][example_data['2']==1],\n",
    "           example_data['2'][example_data['2']==1],facecolors='b')\n",
    "\n",
    "# regne ut beslutningsplanet\n",
    "x = y = np.arange(-4, 4, 0.5)\n",
    "X, Y  = np.meshgrid(x, y)\n",
    "def model2d(c,b,x,y):\n",
    "    return logistic_model(c[0]*x + c[1]*y + b)\n",
    "Z = model2d(clf_log.coef_[0],clf_log.intercept_,X,Y)\n",
    "\n",
    "# plotte beslutningsplanet som en wireframe\n",
    "ax.plot_wireframe(X, Y, Z,color ='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi må importere litt for å kunne bruke hjelpefunksjoner\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.split(os.path.abspath(os.getcwd()))[0])\n",
    "from useful_tools import plot_boundary\n",
    "\n",
    "# skru av interaktive plot\n",
    "% matplotlib inline\n",
    "\n",
    "plt.figure() # ny figur\n",
    "plot_boundary(example_data[['0','1']],clf_log,padding=0.1,plot_step=0.005) # tegne grense\n",
    "plt.scatter(example_data[['0']], example_data[['1']], marker='o', c=example_data['2']) # tegne scatterplot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden logistisk regresjon alltid terskler, hvilket fungerer som et slags \"kutt\", så vil beslutningsgrensa **alltid bli en rett strek**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistisk regresjon i kode\n",
    "\n",
    "Scikit learn har en ferdig pakke for logistisk regresjon også. Den heter `LogisticRegression` og importeres fra `sklearn.linear_model`. Du kan lage en logistisk regresjon klassifikator med `LogisticRegression()` og trene den som før med `.fit(x,y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oppgave 8** Tren en logistisk regresjon kode på irisdatasettet. \n",
    "\n",
    "*Irisdatasettet er et datasett med egenskaper for klassifisering av irisblomster. Datasettet ble først introdusert av en biolog og matematiker ved navn Ronald Fisher i 1936 og har siden den gang blitt brukt gjentatte ganger for å teste og visualisere maskinlæringsteknikker. Hvis du vil vite mer om irisdatasettet kan du lese mer <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">her</a>. Nå er det din tur til å gå i generasjoner med maskinlæringsekseperter sine fotspor og prøve å klassifisere irisblomstene ut fra lengde og bredde på kronbladene *\n",
    "\n",
    "**Hint:**\n",
    "Første steg er å lese inn datasettet fra `'..\\..\\datasets\\iris-species\\Iris.csv'`og hente ut kollonene med egenskapene `SepalWidthCm` og `PetalLengthCm`. Du må også hente ut kollonen `'Species'` som inneholder klassen. Så må du splitte opp i trening og testdata. Deretter er koden ganske lik som det vi gjorde for KNN, men husk å importere og bruke LogisticRegression. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### din kode her: ######\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
